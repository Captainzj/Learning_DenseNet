# -*- coding:utf-8 -*-
'''
[nn.Conv2d简单说明](https://blog.csdn.net/g11d111/article/details/82665265#_nnConv2d_27)
[pytorch方法测试——卷积（二维）](https://blog.csdn.net/tmk_01/article/details/80671335)
'''
import torch
import torch.nn as nn


m = nn.Conv2d(2, 2, kernel_size=3, stride=2)
input = torch.randn(1, 2, 5, 7) # 四维数组
output = m(input)

print("输入图片(2张)：")
print(input)   # size: (1, 2, 5, 7)
print("卷积的权重：")
print(m.weight)  # size: (2, 2, 3, 3)  # (out_channels, in_channels, kernel_size[0], kernel_size[1])
print("卷积的偏重：")
print(m.bias)  # size: (1, 2)  # (out_channels)

print("二维卷积后的输出：")
print(output)   # size: (1, 2, 2, 3)  # o = ( i - kernel_size + 2*Padding)/stride +1
print("输出的尺度：")
print(output.size())

convBlockOne = 0
convBlockTwo = 0
for i in range(3):
    for j in range(3):
        # 第一个卷积核与图片对应相乘
        convBlockOne += m.weight[0][0][i][j] * input[0][0][i][j] \
                        + m.weight[0][1][i][j] * input[0][1][i][j]
        # 第二个卷积核与图片对应相乘
        convBlockTwo += m.weight[1][0][i][j] * input[0][0][i][j] \
                            + m.weight[1][1][i][j] * input[0][1][i][j]
convBlockOne += m.bias[0]
convBlockTwo += m.bias[1]
print("第一个卷积核的输出：")
print(convBlockOne)
print("第二个卷积核的输出：")
print(convBlockTwo)

'''
输入图片(2张)：
tensor([[[[-1.1427,  0.1639, -0.0728,  1.0323, -0.0063, -0.1108, -2.6347],
          [ 1.3359,  0.0467,  0.0399,  0.2122,  1.0498,  0.1918,  0.0366],
          [-1.7654,  1.9370, -0.1885,  1.5327, -1.8388,  1.0487, -1.7247],
          [ 0.1910, -0.4042, -0.0727, -0.5603, -1.2060,  1.5148, -0.3339],
          [-1.1486,  0.1798,  0.6641,  0.2509,  0.5477, -0.0107,  0.2430]],

         [[-0.4018, -0.1993,  0.4881,  0.1094, -0.2965,  1.5645,  1.3194],
          [-0.6477,  0.5723,  0.6518, -1.1087, -1.0365, -0.5499,  1.1762],
          [-1.8791,  0.0608, -0.2641, -0.4145,  0.8355,  0.2747,  0.5411],
          [ 0.2535, -0.5945, -0.6270, -0.9806,  2.0363, -1.4976, -0.7497],
          [-1.2154, -1.0998,  0.7672,  0.0482, -0.7869, -1.1714,  2.1312]]]])
卷积的权重：
Parameter containing:
tensor([[[[-0.1518,  0.2224, -0.2346],
          [ 0.0720, -0.1201, -0.0929],
          [ 0.1603,  0.0085, -0.2151]],

         [[-0.2135, -0.1039,  0.2048],
          [ 0.1655,  0.0203,  0.1647],
          [ 0.0395,  0.1346,  0.0787]]],


        [[[ 0.2067,  0.1389, -0.2049],
          [ 0.0023, -0.1788,  0.1589],
          [-0.1432, -0.1756,  0.2142]],

         [[-0.0261, -0.1398,  0.1776],
          [-0.1755, -0.1571,  0.1039],
          [ 0.2274,  0.0685,  0.2322]]]], requires_grad=True)
卷积的偏重：
Parameter containing:
tensor([0.0576, 0.0268], requires_grad=True)
二维卷积后的输出：
tensor([[[[ 0.2768,  0.2958,  1.0807],
          [ 0.6758,  1.4804,  0.8380]],

         [[-0.5663, -0.3735,  0.9821],
          [ 0.1096,  1.1479, -0.1338]]]], grad_fn=<MkldnnConvolutionBackward>)
输出的尺度：
(1, 2, 2, 3)
第一个卷积核的输出：
tensor(0.2768, grad_fn=<AddBackward0>)
第二个卷积核的输出：
tensor(-0.5663, grad_fn=<AddBackward0>)
'''

'''
这个大家基本都懂，就不多说了，只说一点，

        [[[-0.1518,  0.2224, -0.2346],
          [ 0.0720, -0.1201, -0.0929],
          [ 0.1603,  0.0085, -0.2151]],

         [[-0.2135, -0.1039,  0.2048],
          [ 0.1655,  0.0203,  0.1647],
          [ 0.0395,  0.1346,  0.0787]]],

  为一个卷积核(2*3*3)，而不是
        [[-0.1518,  0.2224, -0.2346],
         [ 0.0720, -0.1201, -0.0929],
         [ 0.1603,  0.0085, -0.2151]],
  为一个卷积核。
'''